---
title: 'Predicting Premature Death Based on Health and Socio-economic Factors'
author: 'Team Illinois: Data Analysis Project Report'
date: '08/07/2020'
output:
  html_document: 
    theme: readable
    toc: yes
  pdf_document: default
urlcolor: cyan
---
***

**Team Illinois Members**

  - Julia Dawson, NETID: juliaed3
  
  - Joseph Stewart, NETID: TODO
  
  - Saumya Singh, NETID: saumyas2
  
***

```{r setup, include=FALSE}

# Read as packages here
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(readxl)
library(dplyr)
library(faraway)
library(lmtest)
library(MASS)
library(knitr)
library(kableExtra)
```

# Introduction


# Methods

## Approach 1

## Approach 2

## Approach 3
### Data Cleaning: 
We started with a script that merged two sheets from the original data. The team chose to keep variables that contained rates and to drop race demographics. We started with 92 variables and 3141 observations. For this approach, we then dropped columns with too many NA's (more than our result YPLLRate) and then omitted observations containing 'NA'. This left us with 71 variables and 2333 observations. Lastly, several variables were coerced to factors and short names were added.

### Test Statistics used:
**adjusted R-squared** is a modified version of R-squared that has been adjusted for the number of predictors in the model. The adjusted R-squared increases only if the new term improves the model more than would be expected by chance. It decreases when a predictor improves the model by less than expected by chance. It indicates the percent of the variation in the output variables that are explained by the input variables.

**LOOCV RMSE**: An observation is removed and the model is fit the the remaining data and this fit used to predict the value of the deleted observation. This is repeated, n times, for each of the n observations and the mean square error is computed.

**Standard Deviation**: We divided LOOCV RMSE by standard deviation in order to normalize RMSE. This allowed us to compare RMSE between models, including models with transformed results.  All three models applied a different transformation to their result.

**Correlation Coefficients** were used to assess how highly correlated predictors are with the result. They were also used to consider which predictors may be interacting.

**Partial Correlation Coefficients** The partial correlation coefficient was used in predictor selection in addition to correlation because it it controls for confounding variables in a model.

**Shapiro-Wilk Test**: Shapiro test assesses the normality of errors. The null hypothesis assumes data were sampled from a normal distribution. A small p-value rejects the null hypothesis. 

**Breusch-Pagan-Godfrey Test**: The null hypothesis for the bptest is constant variance (homoscedasticity). As always if the p-value is below a threshold ($\alpha = 0.05$) then heteroscedasticity is assumed. Heteroscedasticity occurs when the variance of the error terms differ across observations. The BP test statistic how big the problem is. Smaller is better.

### Model Search:
**Result selection:** We chose YPLLRate as our response variable because it represented the overall data and it had the most highly correlated predictors. YPLL is the years of potential life lost before age 75 per 100,000 population (age-adjusted). YPLLRate is a measure of premature death and is used on an interactive site <https://www.countyhealthrankings.org/explore-health-rankings> to allow people to compare their State and County to others. The statement is on their home page is interesting... _"The annual Rankings provide a revealing snapshot of how health is influenced by where we live, learn, work, and play. They provide a starting point for change in communities."_ . Data is gathered from a variety of sources to create a model for comparison between States and Counties (<https://www.countyhealthrankings.org/explore-health-rankings/measures-data-sources/2020-measures>). Several other variables were considered to be the result but YPLLRate was chosen as the best representative of the data.  

**Predictor selection:** 
  + Predictors were chosen and tested for Model 3 if;
    - they had some correlation with result (YPLLRate).
    - their partial correlation with result (YPLLRate) was high while controlling for the other model predictors.
    - they showed up in several large automatically generated models (full additive, full interactive, backward AIC)
    - they were representative of factor we thought could be significant
    
  + Predictors were eliminated one by one if:
    - they had high p-values in a model
    - removal had good or low affect on adjusted R-squared and LOOCV-RMSE
    - predictor was highly correlated with another predictor in the model
    
  + Automatic Model builds that were used for comparison:
    - Additive model all and backward AIC
    - Multiplicative model all and backward AIC
    - regsubsets all possible models (we tried this but it ran for three days and crashed the computer)
    
**Problems with Model_3 and methods to improve it:**
  + LOOCV RMSE (Cross-Validated RMSE) was too high (355)
    - Outliers were removed and improved LOOCV RMSE by 30%.
    - Transforming a few predictors with log and higher order terms improved the adjusted-rsquared and LOOCV RMSE.
    - Adding some two-way interactions improved the adjusted r-squared and LOOCV RMSE slightly.
    - BoxCox recommended a lambda of 0.8. Transformation of response $YPLLRate^\lambda$ produced the best results (rather than $Y^\lambda$ rather than $\frac{(Y^\lambda - 1)}\lambda$). It improved LOOCV RMSE 50% and improved the normality problems with response YPLLRate. The LOOCV RMSE was well below the standard deviation of YPLLRate. The large tails on the Q-Q plot shrank to almost nothing.
    
  + Model suffers from heteroscedasticity (Breusch-Pagan Test) and correlation between residuals (Durbin Watson Test)     - Common fixes for heteroscedasticity are to transform the response variable (log or sqrt) or to use weighted regression.
    - scaled the values of predictors - this did not help
    - Weighted regression did not improve the BP test statistic.
    - Remove predictors one by one with highest p-value. This improved all measures.
    - Removing influential observations improved LOOCV RMSE 30%.
    - Transforming the Response YPLLRate reduced BP value 50% but is still too high.
    - we tried a model that does these automatically like glmnet but the results and model were difficult to interpret and compare in this short timeframe.
    
### Assessment of Model 3:
Model 3 has a good adjusted R-squared value of 0.972.  LOOCV RMSE is 17.5 and when normalized with the standard deviation the normalized RMSE is 0.16. This is a pretty good value.  When the data was split into test and train and run 1000 times, the normalized RMSE was 0.17, confirming our LOOCV RMSE result. The transformation with BoxCox lambda greatly improved the Q-Q plot and the tails were minimal. The Shapiro test had a high P-value which support the null hypothesis of normality of errors.  The remaining problem is it fails the BPtest with BP value of 131 and p-value very low. The model still suffers from heteroscedasticity but does well on the other measures.  BP value was much improved from over 300 before the BoxCox transform and removal of influential observations. Model 3 uses 23 predictors out of 93 possible and they are representative of the major contributors to health (environment, income, healthcare, demographics). Given the high correlation between many of the predictors and the high variance of the response, this is a decent model.
```{r, eval=T, echo=F}

#model_1 = lm(Employed ~ . , data = longley)

#Read data for Model 3
Raw95naomit = read.csv(".\\data\\DataCleanedJulia.csv") 
#str(Raw95naomit)
#Force character arrays to be factors.  
Raw95naomit$State = as.factor(Raw95naomit$State)
Raw95naomit$Region= as.factor(Raw95naomit$Region)
Raw95naomit$WaterViol = as.factor(Raw95naomit$WaterViol)
#Make this model to can figure out the lambda
mdl3t = lm(formula = YPLLRate ~ HealthPoorPct + 
             FoodEnvirIX + PhysInactPct + 
             PhysPrimCareRate + 
             MammAnnualPct +   
             Income20thpcntl + IncomeRatio + SocsAssRate + 
             InjuryDeathRate + HousSvrProbPct + 
             HousInadFacil + LifeExpect + 
             DeathAgeAdjRate + VaccinatedPct + 
             IncHousehldMedian + 
             PopGE65Pct + PopNativePct + PopPacificPct + 
             EnglishNotProfPct + PopRuralPct
           + PopGE65Pct:DeathAgeAdjRate
           + HealthPoorPct:PopFemalePct
           + LBWPct:IncHousehldMedian
           , data = Raw95naomit)

#Transform according to BoxCox lambda recommendation
lambda_mdl3 = boxcox(mdl3t, plotit = FALSE) #lambda a little less than 0.8
lmda_mdl3 = lambda_mdl3$x[which.max(lambda_mdl3$y)]

#Detect outliers
cdnaomit = cooks.distance(mdl3t)

#Create Model_3
model_3 = lm(formula = YPLLRate^lmda_mdl3 ~ HealthPoorPct + 
             FoodEnvirIX + PhysInactPct + 
             PhysPrimCareRate + 
             MammAnnualPct +   
             Income20thpcntl + IncomeRatio + SocsAssRate + 
             InjuryDeathRate + HousSvrProbPct + 
             HousInadFacil + LifeExpect + 
             DeathAgeAdjRate + VaccinatedPct + 
             IncHousehldMedian + 
             PopGE65Pct + PopNativePct + PopPacificPct + 
             EnglishNotProfPct + PopRuralPct
           + PopGE65Pct:DeathAgeAdjRate
           + HealthPoorPct:PopFemalePct
           + LBWPct:IncHousehldMedian
           , data = Raw95naomit, 
           subset = cdnaomit <= 4/length(cdnaomit))
```
# The three models are shown below:

**Model_1**
```{r}
formula(model_3)
```

**Model_2**
```{r}
formula(model_3)
```
**Model_3**
```{r}
formula(model_3)
```


# Model Diagnostics

```{r, eval=T, echo=F}

plot_fitted_resid = function(model, pointcol = "dodgerblue", linecol = "darkorange", title) {
  
  plot(fitted(model), resid(model), col = pointcol, pch = 20, cex = 1.5, xlab = "Fitted", ylab = "Residuals", main = title)
  abline(h = 0, col = linecol, lwd = 2)
}

plot_qq = function(model, pointcol = "dodgerblue", linecol = "darkorange", title) {
  qqnorm(resid(model), main = title, col = pointcol, pch = 20, cex = 1.5)
  qqline(resid(model), col = linecol, lwd = 2)
}

plot_hist_resid = function(model, pointcol = "dodgerblue", linecol = "darkorange", title) {
  hist(resid(model),
       xlab   = "Residuals",
       main   = title,
       col    = pointcol,
       border = linecol,
       breaks = 45)
}

```

**Residuals Plot**
```{r fig.height=8, fig.width=20}
par(mfrow = c(1,3))
#plot_fitted_resid(model_1, title="Model 1")
#plot_fitted_resid(model_2, title="Model 2")
plot_fitted_resid(model_3, title="Model 3")

```

**Q-Q Plot**
```{r}
par(mfrow = c(1,3))
plot_qq(model_3, title="Model 3")
```




# Results


# Discussion

```{r, eval=T, echo=F}

get_bp = function(model) {
  bptest(model)$p.value
  #decide = unname(bptest(model)$p.value < alpha)
  #ifelse(decide, "Reject", "Fail to Reject")
}

get_sw = function(model) {
  shapiro.test(resid(model))$p.value
  #decide = unname(shapiro.test(resid(model))$p.value < alpha)
  #ifelse(decide, "Reject", "Fail to Reject")
}

get_num_params = function(model) {
  length(coef(model))
}

get_loocv_rmse = function(model) {
  sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}

get_adj_r2 = function(model) {
  summary(model)$adj.r.squared
}

get_rmse = function(model) {
  sqrt(mean(resid(model) ^ 2))
}

get_AIC = function(model) {
  extractAIC(model)[2]
}
#Joe and Saumya - we need to normalize RMSE to compare so each of us needs to supply an stddev and point to your YPLLRate
rmse_norm_1 = get_rmse(model_3)/sd(log(Raw95naomit$YPLLRate)) #Model 1 result transform
info_1 = list(get_bp(model_3), get_sw(model_3), get_num_params(model_3), 
              get_loocv_rmse(model_3), get_adj_r2(model_3), get_rmse(model_3), 
              rmse_norm_1, get_AIC(model_3))

rmse_norm_2 = get_rmse(model_3)/sd((Raw95naomit$YPLLRate^lmda_mdl3 - 1)/lmda_mdl3) #Model 2 result transform
info_2 = list(get_bp(model_3), get_sw(model_3), get_num_params(model_3), 
              get_loocv_rmse(model_3), get_adj_r2(model_3), get_rmse(model_3), 
              rmse_norm_2, get_AIC(model_3))

rmse_norm_3 = get_rmse(model_3)/sd(Raw95naomit$YPLLRate^lmda_mdl3)
info_3 = list(get_bp(model_3), get_sw(model_3), get_num_params(model_3), 
              get_loocv_rmse(model_3), get_adj_r2(model_3), get_rmse(model_3), 
              rmse_norm_3, get_AIC(model_3))

model_info = cbind(info_1, info_2, info_3)

rownames(model_info) = c("BP Test", "Shapiro","No params", "LOOV RMSE", "Adj R2", "RMSE", "norm RMSE", "AIC")
colnames(model_info) = c("Model 1", "Model 2", "Model 3")
knitr::kable(model_info, digits=3) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))

```


# Appendix
